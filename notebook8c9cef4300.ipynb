{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport tqdm\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-17T09:27:42.081323Z","iopub.execute_input":"2021-08-17T09:27:42.081656Z","iopub.status.idle":"2021-08-17T09:27:42.087274Z","shell.execute_reply.started":"2021-08-17T09:27:42.081625Z","shell.execute_reply":"2021-08-17T09:27:42.086469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torchvision.transforms as transforms\nfrom torchvision.models import vgg19\nfrom pathlib import Path\nimport PIL\nfrom time import time\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:42.114985Z","iopub.execute_input":"2021-08-17T09:27:42.115234Z","iopub.status.idle":"2021-08-17T09:27:42.119887Z","shell.execute_reply.started":"2021-08-17T09:27:42.115209Z","shell.execute_reply":"2021-08-17T09:27:42.118811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"../input/landmark-recognition-2021\"\ntrain_dir = data_dir + \"/train\"\ntest_dir = data_dir + \"/test\"\ntrain_file =  \"../input/landmark-recognition-2021/train.csv\"\nsub_file = data_dir +\"/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:42.145019Z","iopub.execute_input":"2021-08-17T09:27:42.145266Z","iopub.status.idle":"2021-08-17T09:27:42.149426Z","shell.execute_reply.started":"2021-08-17T09:27:42.145242Z","shell.execute_reply":"2021-08-17T09:27:42.148625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_file)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:42.179854Z","iopub.execute_input":"2021-08-17T09:27:42.180092Z","iopub.status.idle":"2021-08-17T09:27:43.040519Z","shell.execute_reply.started":"2021-08-17T09:27:42.18007Z","shell.execute_reply":"2021-08-17T09:27:43.039523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df.landmark_id.unique())","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:43.042119Z","iopub.execute_input":"2021-08-17T09:27:43.042487Z","iopub.status.idle":"2021-08-17T09:27:43.061997Z","shell.execute_reply.started":"2021-08-17T09:27:43.042449Z","shell.execute_reply":"2021-08-17T09:27:43.061091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landmark_ids = {lid:i for i, lid in enumerate(train_df.landmark_id.unique())}","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:43.063719Z","iopub.execute_input":"2021-08-17T09:27:43.064064Z","iopub.status.idle":"2021-08-17T09:27:43.112019Z","shell.execute_reply.started":"2021-08-17T09:27:43.064027Z","shell.execute_reply":"2021-08-17T09:27:43.111205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MakeData(Dataset):\n    def __init__(self,csv_file,data_dir,transform = None,data_mode = 'train'):\n        super(MakeData,self).__init__()\n        self.csv_file = pd.read_csv(csv_file)\n        if data_mode == 'train':\n            self.landmark_ids = {ids:i for i,ids in enumerate(self.csv_file.landmark_id.unique())}\n            self.csv_file['landmark_id'] = self.csv_file['landmark_id'].map(self.landmark_ids)\n        \n        self.data_dir = data_dir\n        self.transform = transform\n        \n    def __getitem__(self,ids):\n        if torch.is_tensor(ids):\n            idx = idx.tolist()\n        img_id = self.csv_file.iloc[ids, 0]\n        img_class = self.csv_file.iloc[ids, 1]\n        img_path = os.path.join(self.data_dir, img_id[0], img_id[1], img_id[2], f'{img_id}.jpg')\n        img = Image.open(img_path)\n        if self.transform is not None:\n            img = transform(img)\n        sample = [img, img_class, img_id]\n        return sample\n    def __len__(self):\n        return len(self.csv_file)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:43.11351Z","iopub.execute_input":"2021-08-17T09:27:43.113873Z","iopub.status.idle":"2021-08-17T09:27:43.123444Z","shell.execute_reply.started":"2021-08-17T09:27:43.113836Z","shell.execute_reply":"2021-08-17T09:27:43.122504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([ transforms.CenterCrop(224), \n                               transforms.ToTensor()])\ntrain_ = MakeData(train_file, train_dir, transform, \"train\")\ntest_ = MakeData(sub_file, test_dir, transform, \"test\")","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:43.124926Z","iopub.execute_input":"2021-08-17T09:27:43.125463Z","iopub.status.idle":"2021-08-17T09:27:44.117284Z","shell.execute_reply.started":"2021-08-17T09:27:43.125423Z","shell.execute_reply":"2021-08-17T09:27:44.116478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_size = 0.2\nbatch_size = 128","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:44.120265Z","iopub.execute_input":"2021-08-17T09:27:44.120528Z","iopub.status.idle":"2021-08-17T09:27:44.126221Z","shell.execute_reply.started":"2021-08-17T09:27:44.1205Z","shell.execute_reply":"2021-08-17T09:27:44.125414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train = len(train_)\nindices = list(range(num_train))\nnp.random.seed(100)\nnp.random.shuffle(indices)\nsplit = int(np.floor(num_train*valid_size))\nvalid_idx, train_idx = indices[:split], indices[split:]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:44.127738Z","iopub.execute_input":"2021-08-17T09:27:44.12811Z","iopub.status.idle":"2021-08-17T09:27:44.450465Z","shell.execute_reply.started":"2021-08-17T09:27:44.128073Z","shell.execute_reply":"2021-08-17T09:27:44.449634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\ntrain_loader = DataLoader(train_, batch_size=batch_size, sampler=train_sampler, num_workers=0)\nvalid_loader = DataLoader(train_, batch_size=batch_size, sampler=valid_sampler, num_workers=0)\ntest_loader = DataLoader(test_, batch_size=batch_size, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:44.453026Z","iopub.execute_input":"2021-08-17T09:27:44.453393Z","iopub.status.idle":"2021-08-17T09:27:44.458091Z","shell.execute_reply.started":"2021-08-17T09:27:44.453354Z","shell.execute_reply":"2021-08-17T09:27:44.4573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nuse_cuda","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:44.459778Z","iopub.execute_input":"2021-08-17T09:27:44.460378Z","iopub.status.idle":"2021-08-17T09:27:44.470271Z","shell.execute_reply.started":"2021-08-17T09:27:44.460341Z","shell.execute_reply":"2021-08-17T09:27:44.469288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_transfer = vgg19(pretrained=True)\nfor params in model_transfer.features.parameters():\n    params.requires_grad=False\nin_features = model_transfer.classifier[6].in_features\nlast_layer = nn.Linear(in_features,  train_df.landmark_id.nunique())\nmodel_transfer.classifier[6] = last_layer\nmodel_transfer.cuda()\nprint('modelDone')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:44.471725Z","iopub.execute_input":"2021-08-17T09:27:44.472091Z","iopub.status.idle":"2021-08-17T09:27:49.207289Z","shell.execute_reply.started":"2021-08-17T09:27:44.472053Z","shell.execute_reply":"2021-08-17T09:27:49.206368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_transfer","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.208706Z","iopub.execute_input":"2021-08-17T09:27:49.209069Z","iopub.status.idle":"2021-08-17T09:27:49.21724Z","shell.execute_reply.started":"2021-08-17T09:27:49.209029Z","shell.execute_reply":"2021-08-17T09:27:49.216141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_transfer.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.218992Z","iopub.execute_input":"2021-08-17T09:27:49.219544Z","iopub.status.idle":"2021-08-17T09:27:49.227284Z","shell.execute_reply.started":"2021-08-17T09:27:49.21948Z","shell.execute_reply":"2021-08-17T09:27:49.226292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaders = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.228753Z","iopub.execute_input":"2021-08-17T09:27:49.229245Z","iopub.status.idle":"2021-08-17T09:27:49.348163Z","shell.execute_reply.started":"2021-08-17T09:27:49.229206Z","shell.execute_reply":"2021-08-17T09:27:49.347149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"../working/\"\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.349636Z","iopub.execute_input":"2021-08-17T09:27:49.350244Z","iopub.status.idle":"2021-08-17T09:27:49.358769Z","shell.execute_reply.started":"2021-08-17T09:27:49.350202Z","shell.execute_reply":"2021-08-17T09:27:49.357786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 20","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.360037Z","iopub.execute_input":"2021-08-17T09:27:49.360603Z","iopub.status.idle":"2021-08-17T09:27:49.369213Z","shell.execute_reply.started":"2021-08-17T09:27:49.36054Z","shell.execute_reply":"2021-08-17T09:27:49.368306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path, num_batch=1, verbose=False):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf \n    \n    for epoch in range(1, n_epochs+1):\n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        start_time = time()\n        \n        ###################\n        # train the model #\n        ###################\n        # set the module to training mode\n        model.train()\n#         import pdb; pdb.set_trace()\n        for batch_idx, (data, target, img_id) in enumerate(loaders['train']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n\n            ## TODO: find the loss and update the model parameters accordingly\n            ## record the average training loss, using something like\n            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n            optimizer.zero_grad()\n            out = model(data)\n            loss = criterion(out, target)\n            loss.backward()\n            optimizer.step()\n            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n            train_loss += loss.data.item()*data.size(0)\n            if verbose:\n                print(f\"idx: {batch_idx} Train Loss:{train_loss/(data.size(0) * (batch_idx + 1)) : .6f}\")\n            if batch_idx > num_batch:\n                train_images_used = data.size(0)*(batch_idx + 1)\n                break\n\n            \n        torch.cuda.empty_cache()\n        ######################    \n        # validate the model #\n        ######################\n        # set the model to evaluation mode\n        model.eval()\n        for batch_idx, (data, target, img_id) in enumerate(loaders['valid']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n\n            ## TODO: update average validation loss \n            out = model(data)\n            loss = criterion(out, target)\n            valid_loss += loss.data.item()*data.size(0) \n            if verbose:\n                print(f\"idx: {batch_idx} Valid Loss:{valid_loss / (data.size(0) * (batch_idx + 1)) : .6f}\")\n            if batch_idx > num_batch:\n                valid_images_used = data.size(0)*(batch_idx + 1)\n                break\n        train_loss = train_loss/ train_images_used\n        valid_loss = valid_loss / valid_images_used\n\n            \n            \n        end_time = time()\n        time_taken = end_time - start_time\n        # print training/validation statistics \n        print('Epoch: {} \\t Time: {:.2f} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n            epoch, \n            time_taken,\n            train_loss,\n            valid_loss\n            ))\n\n        ## TODO: if the validation loss has decreased, save the model at the filepath stored in save_path\n        if valid_loss < valid_loss_min:\n            if verbose:\n                print(f\"Valid loss reduced from {valid_loss_min :.6f} to {valid_loss :.6f}, saving model\")\n            valid_loss_min = valid_loss\n            torch.save(model.state_dict(), save_path)\n              \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.37075Z","iopub.execute_input":"2021-08-17T09:27:49.371166Z","iopub.status.idle":"2021-08-17T09:27:49.486932Z","shell.execute_reply.started":"2021-08-17T09:27:49.371119Z","shell.execute_reply":"2021-08-17T09:27:49.485686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(loaders, model, use_cuda, landmark_reverse_map):\n    \n    \n    # set the module to evaluation mode\n    model.eval()\n    sf = nn.Softmax(dim=1)\n    img_id_list = []\n    confidence_list = []\n    label_list = []\n    tot_batch = len(loaders['test'])\n    for batch_idx, (data, _, img_id) in enumerate(tqdm.tqdm(loaders['test'])):\n        # move to GPU\n        if use_cuda:\n            data = data.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        output = sf(output)\n        output = torch.max(output, dim=1)\n        confidence = output[0].cpu().detach().numpy()\n        label=output[1].cpu().detach().numpy()\n        \n        img_id_list.extend(list(img_id))\n        confidence_list.extend(confidence.tolist())\n        label_list.extend(label.tolist())\n    \n    predict_df = pd.DataFrame({'id': img_id_list, \n                               'landmarks': label_list, \n                               'conf': confidence_list})\n    predict_df['landmarks'] = predict_df['landmarks'].map(landmark_reverse_map)\n    predict_df['landmarks'] = predict_df['landmarks'].astype(str) +\" \" + predict_df['conf'].round(6).astype(str)\n\n    predict_df.drop(\"conf\", axis=1, inplace=True)\n    return predict_df","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:27:49.488441Z","iopub.execute_input":"2021-08-17T09:27:49.489038Z","iopub.status.idle":"2021-08-17T09:27:49.500601Z","shell.execute_reply.started":"2021-08-17T09:27:49.488998Z","shell.execute_reply":"2021-08-17T09:27:49.499816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nmodel_learn = train(num_epochs, loaders, model_transfer, optimizer, \n                      loss_fn, use_cuda, os.path.join(save_path,'model_transfer.pt'),num_batch=1000, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T09:28:13.915438Z","iopub.execute_input":"2021-08-17T09:28:13.915802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}